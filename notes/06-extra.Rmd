# Logistic Regression Combination

```{r echo=FALSE}
data(gambia)
X <- as.matrix(gambia[, c(4:8)])
X <- cbind(intercept = 1, X) # Add intercept column to design matrix
Y <- gambia[, 3] # Response variable
n <- length(Y)

mle <- glm(Y ~ X - 1, family = "binomial") # Logistic regression using MLE
b <- mle$coefficients
summary(mle)

# Classify to 1 with probability
q <- function(x, b) {
  exp(b %*% x) / (1 + exp(b %*% x))
}

# (proportional to) log posterior in the indep normals prior case
logpost <- function(b, X, mu, sigma) {
  logprior <- sum((b - mu)^2 / 2*sigma)
  nu <- apply(X, 1, function(x) b %*% x) # Vector of linear predictors
  loglike <- sum(nu[Y == 1]) + sum(-log(1 + exp(nu)))
  logprior + loglike
}

# Function to update jth component of beta via RWMH step
mh <- function(b, X, mu, sigma, j, scale) {
  y <- b
  y[j] <- y[j] + rnorm(1, mean = 0, sd = scale)
  a <- exp(logpost(y, X, mu, sigma) - logpost(b, X, mu, sigma)) # Acceptance probability
  if(a > runif(1)) return(y) else return(b)
}

# Metropolis-within-Gibbs sampler (random scan by default)
mwg <- function(b0, X, mu, sigma, scale, nsim, random_scan = TRUE) {
  p <- length(b0)
  r <- array(NA, c(nsim, p)) # For the chain
  r[1, ] <- b0 # Init chain
  s <- array(0, c(3, p)) # For acceptance rates
  for(i in 2:nsim) {
    if(random_scan) {
      j <- sample(1:p, 1) # Random scan
    } else {
      j <- (i %% p) + 1 # Systematic scan
    }
    s[1, j] <- s[1, j] + 1 # Update pick count
    r[i, ] <- mh(r[i-1, ], X, mu, sigma, j, scale[j])
    if(!identical(r[i, ], r[i-1, ])) s[2, j] <- s[2, j] + 1 # Update accept count
  }
  r <- as.data.frame(r)
  names(r) <- sprintf("b%d", 0:(p-1))
  s[3, ] <- s[2, ] / s[1, ] # Acceptance rates
  return(list("chain" = r, "accept" = s))
}

my_guess <- c(0.175, 0.00025, 0.2, 0.3, 0.005, 0.2)
```

## Submodels

|   | $\text{Intercept}$ | $\beta_1$ | $\beta_2$ | $\beta_3$ | $\beta_4$ | $\beta_5$ |
|---|:----:|:--:|:--:|:--:|:--:|:--:|
| Submodel 1 | $\checkmark$  | $\checkmark$  |   |   | $\checkmark$  | $\checkmark$  |
| Submodel 2 | $\checkmark$  |   | $\checkmark$  | $\checkmark$  |   | $\checkmark$  |


The submodels $\mathcal{M}_1$ and $\mathcal{M}_2$ are defined by linear predictors 
$$
\eta_1 = \beta_{0} + \beta_1 \cdot \texttt{age} +  \beta_4 \cdot \texttt{green} + \beta_5 \cdot \texttt{phc},
$$

and
$$
\eta_2 = \beta_{0} + \beta_2 \cdot \texttt{netuse} + \beta_3 \cdot \texttt{treated} + \beta_5 \cdot \texttt{phc}.
$$

The link parameter is $\phi = (\beta_0, \beta_5)$ and model specific parameters are $\psi_1 = (\beta_1, \beta_4)$ and $\psi_2 = (\beta_2, \beta_3)$. Both submodels have the same observable random variables $Y_1 = Y_2$ the response variable `pos`.

Define $q_k$ for $k = 1, 2$ by
$$
q_k(x) = \frac{\exp \left(\eta_k\right)}{1+ \exp\left(\eta_k\right)}
$$

First we will run the submodels seperately.

### Maximum-likelihood

```{r}
X1 <- X[, c(1, 2, 5, 6)] # Design matrix 1: intercept, age, green and phc
p1 <- 4

X2 <- X[, c(1, 3, 4, 6)] # Design matrix 2: intercept, netuse, treated and phc
p2 <- 4

# GLM for model 1
mle1 <- glm(Y ~ X1 - 1, family = "binomial")
summary(mle1)

b1 <- mle1$coefficients

# GLM for model 2
mle2 <- glm(Y ~ X2 - 1, family = "binomial")
summary(mle2)

b2 <- mle2$coefficients
```

### Bayesian

Take the same normal prior as before for $\beta_{0:5}$.

```{r}
# Priors same as in the joint modelling to start with (so consistent in the link parameter)
mu1 <- rep(0, p1)
sigma1 <- rep(0.1, p1)

mu2 <- rep(0, p2)
sigma2 <- rep(0.1, p2)
```

```{r eval=FALSE}
model1 <- mwg(b0 = rep(0, 4), X = X1, mu = mu1, sigma = sigma1,
               scale = my_guess[c(1, 2, 5, 6)], nsim = 10^5)
names(model1$chain) <- c("b0", "b1", "b4", "b5") # Correct the naming

model2 <- mwg(b0 = rep(0, 4), X = X2, mu = mu2, sigma = sigma2,
               scale = my_guess[c(1, 3, 4, 6)], nsim = 10^5)
names(model2$chain) <- c("b0", "b2", "b3", "b5") # Correct the naming
```

```{r echo=FALSE}
model1 <- readRDS(file = "../output/model1.Rds")
model2 <- readRDS(file = "../output/model2.Rds")
```

```{r warning=FALSE, message=FALSE}
mcmc_trace(model1$chain) # Traceplots
mcmc_hist(model1$chain) # Histograms
model1$accept
colMeans(model1$chain); b1

mcmc_trace(model2$chain) # Traceplots
mcmc_hist(model2$chain) # Histograms
model2$accept
colMeans(model2$chain); b2
```

## Markov Combination

The submodels have consistent prior marginals in the link parameter and Markov combination can be applied.

The joint distribution corresponding to submodel $\mathcal{M}_1$, as a function of the parameters, is proportional to the posterior, which itself is proportional to the prior times the likelihood

\begin{align*}
p_1(\phi, \psi_1, \mathbf{y}_1) &\propto p_1(\phi, \psi_1 | \mathbf{y}_1) \\
&= p_1(\beta_0, \beta_1, \beta_4, \beta_5 | \mathbf{y}) \\
&\propto \underbrace{\prod_{j=0, 1,4,5}\exp\left(\frac{1}{2\sigma_j^2}(\beta_j - \mu_j)^2\right)}_{\text{Prior on } (\phi, \psi_1) = (\beta_0, \beta_1, \beta_4, \beta_5)} \times
\underbrace{\prod_{i=1}^{2035} q_1\left(x_i\right)^{y_i}\left(1-q_1\left(x_i\right)\right)^{1-y_i}}_{\text{Likelihood}}.
\end{align*}

Similarly for $\mathcal{M}_2$
\begin{align*}
p_2(\phi, \psi_2, \mathbf{y}_2) &\propto p_2(\phi, \psi_2 | \mathbf{y}_2) \\
&= p_2(\beta_0, \beta_2, \beta_3, \beta_5 | \mathbf{y}) \\
&\propto \underbrace{\prod_{j=0, 2,3,5}\exp\left(\frac{1}{2\sigma_j^2}(\beta_j - \mu_j)^2\right)}_{\text{Prior on } (\phi, \psi_1) = (\beta_0, \beta_2, \beta_3, \beta_5)} \times 
\underbrace{\prod_{i=1}^{2035} q_2\left(x_i\right)^{y_i}\left(1-q_2\left(x_i\right)\right)^{1-y_i}}_{\text{Likelihood}}.
\end{align*}

Therefore the Markov combination in this case is
\begin{align*}
p_{\mathrm{comb}}\left(\phi, \psi_1, \psi_2, \mathbf{y}_1, \mathbf{y}_2\right) &= \frac{p_1\left(\phi, \psi_1, \mathbf{y}_1\right)p_2\left(\phi, \psi_2, \mathbf{y}_2\right)}{p(\phi)} \\
&\propto \frac{p_1(\beta_0, \beta_1, \beta_4, \beta_5 | \mathbf{y}) p_2(\beta_0, \beta_2, \beta_3, \beta_5 | \mathbf{y})}{p(\beta_0, \beta_5)} \\
&\propto \prod_{j=0}^5\exp\left(\frac{1}{2\sigma_j^2}(\beta_j - \mu_j)^2\right) \\
&\times \prod_{i=1}^{2035} q_1\left(x_i\right)^{y_i}\left(1-q_1\left(x_i\right)\right)^{1-y_i} q_2\left(x_i\right)^{y_i}\left(1-q_2\left(x_i\right)\right)^{1-y_i} (\#eq:target)
\end{align*}

Informally $p_{\mathrm{comb}}$ contains the product of the priors on the full set of parameters $(\phi, \psi_1, \psi_2)$ with both likelihoods from $\mathcal{M}_1$ and $\mathcal{M}_2$. So this is almost like Bayesian inference for full model but with a different likelihood. It seems as if the information contained by the data $\mathcal{D}$ is being used more than once.

Want to sample from \@ref(eq:target) using the methods described in @goudie2019joining. As before, it is convenient to work in terms of a logarithmic version $\log p_{\mathrm{comb}}\left(\phi, \psi_1, \psi_2, \mathbf{y}_1, \mathbf{y}_2\right)$ which is equal to

$$
\sum_{j=0}^5\frac{1}{2\sigma_j^2}(\beta_j - \mu_j)^2 + \sum_{i=1}^{2035} \{ {y_i}(\log q_1\left(x_i\right) + \log q_2\left(x_i\right))  + (1 - y_i) (\log \left(1-q_1\left(x_i\right)\right) + \log \left(1-q_2\left(x_i\right)\right)\}.
$$
```{r}
logtarget <- function(b, s1, s2, X1, X2, mu1, mu2, sigma1, sigma2) {
  b1 <- b[s1]; b2 <- b[s2]
  as.numeric(logpost(b1, X1, mu1, sigma1) +
             logpost(b2, X2, mu2, sigma2) -
             sum((b1[c(1, 4)] - mu1[c(1, 4)])^2 / 2*sigma1[c(1, 4)]))
}
```

### Metropolis-within-Gibbs

* Update first latent parameter $\psi_1$ by systematic scan MWG where target-to-proposal is identical to that of $\mathcal{M}_1$
* Update second latent parameter $\psi_2$ by systematic scan MWG where target-to-proposal is identical to that of $\mathcal{M}_2$
* Update link parameter $\phi$ by by systematic scan MWG

```{r}
# This is getting a bit complicated

comb_mwg <- function(b0, s1, s2, X, mu, sigma, scale, nsim) { # Systematic-scan
  X1 <- X[, s1]; X2 <- X[, s2]

  mu1 <- mu[s1]; mu2 <- mu[s2]

  sigma1 <- sigma[s1]; sigma2 <- sigma[s2]

  p <- length(b0)
  r <- array(NA, c(nsim, p))
  r[1, ] <- b0 # Init chain

  order <- c(setdiff(s1, s2), setdiff(s2, s1), intersect(s1, s2))

  for (i in 2:nsim) {
    j <- order[((i - 2) %% p) + 1] # Index of the parameter to be updated

    if(j %in% setdiff(s1, s2)) {
      y <- r[i-1, ]
      y[j] <- y[j] + rnorm(1, mean = 0, sd = scale[j])
      a <- exp(logpost(y[s1], X1, mu1, sigma1) - logpost(b[s1], X1, mu1, sigma1))
      if(a > runif(1)) {
        r[i, ] <- y
      } else {
        r[i, ] <- r[i-1, ]
      }
    }

    if(j %in% setdiff(s2, s1)) {
      y <- r[i-1, ]
      y[j] <- y[j] + rnorm(1, mean = 0, sd = scale[j])
      a <- exp(logpost(y[s2], X2, mu2, sigma2) - logpost(b[s2], X2, mu2, sigma2))
      if(a > runif(1)) {
        r[i, ] <- y
      } else {
        r[i, ] <- r[i-1, ]
      }
    }

    if(j %in% intersect(s2, s1)) {
      y <- r[i-1, ]
      y[j] <- y[j] + rnorm(1, mean = 0, sd = scale[j])
      a <- exp(logtarget(y, s1, s2, X1, X2, mu1, mu2, sigma1, sigma2) -
               logtarget(b, s1, s2, X1, X2, mu1, mu2, sigma1, sigma2))
      if(a > runif(1)) {
        r[i, ] <- y
      } else {
        r[i, ] <- r[i-1, ]
      }
    }
  }
  r <- as.data.frame(r)
  names(r) <- sprintf("b%d", 0:(p-1))
  return(r)
}

```

```{r eval=FALSE}
b0 <- rep(0, 6)
s1 <- c(1, 2, 5, 6)
s2 <- c(1, 3, 4, 6)

test <- comb_mwg(b0, s1, s2, X, mu, sigma, scale = my_guess, nsim = 10^5)
```

```{r echo=FALSE}
comb <- readRDS(file = "../output/comb.Rds")
```

```{r}
mcmc_trace(comb)
mcmc_hist(comb)
colMeans(comb)
```

This doesn't look too good...
