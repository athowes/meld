# Model specification \label{chapter:model}

Markov melding is a Bayesian procedure based on the construction and integration of a collection of graphical models; one for each of the separate evidence components.
we begin this chapter by introducing Bayesian statistics in Section \ref{sec:bayes} and outlining how models might be constructed using a Bayesian networks, a type of graphical model, in Section \ref{sec:graph}.

## Bayesian statistics \label{sec:bayes}

Consider observable data $Y \in \mathcal{Y}$.
Statistical models simplify reality by using parameters $\theta \in \Theta$ to explain the processes governing $Y$.
In Bayesian statistics, the unknown quantities such as $Y$ and $\theta$ are treated as random variables.
A statistical model $\mathcal{M}$ is then defined entirely by the joint probability distribution $p(\theta, Y)$, encompassing all beliefs about the parameters, data and their interdependence.

This distribution $p(\theta, Y)$ can be decomposed into a product of the likelihood $p(Y \, | \, \theta)$ and the prior $p(\theta)$ distributions.
Having observed the data $Y = y$, the likelihood function $\mathcal{L}(\theta \, | \, y) = p(y \, | \, \theta)$ expresses how probable the generation of data $y$ is under parameter setting $\theta$.
The prior distribution $p(\theta)$ can be interpreted as embodying subjective judgments about the plausibility of the different parameter settings in advance of seeing the data.
Alternatively, from an objective Bayesian point of view, a prior which is in some sense non-informative about the parameters should be chosen.
In either setting there is significant freedom in prior-specification which is left to the practitioner.

Statistical inference is the process of learning about the underlying parameters once the data $Y = y$ has been observed.
In Bayesian statistics, this is achieved using probability theory via the eponymous Bayes' rule
\begin{equation}
p(\theta \, | \, y) = \frac{p(y \, | \, \theta) p(\theta)}{p(y)}. \label{eq:bayes}
\end{equation}
The posterior distribution $p(\theta \, | \, y)$ represents updated beliefs about the parameters conditional on the observed data.
The marginal likelihood $p(y)$ is a constant which normalises the product $p(y \, | \, \theta)p(\theta)$ to a valid probability distribution which integrates to one.
Calculating the marginal likelihood is often difficult, preventing exact Bayesian inference from being tractable.
However, during this chapter we will concentrate on statistical modelling, mostly ignoring potential computational difficulties for the time being.

Bayes' rule \eqref{eq:bayes} can also be written in the form 
\begin{equation}
p(\theta \, | \, y) \propto p(y \, | \, \theta) p(\theta) = p(y, \theta), \label{eq:propbayes}
\end{equation}
omitting the dependence on the marginal likelihood. 
Indeed, in practice Equation \eqref{eq:propbayes} is the form of Bayes' rule most widely useful.
To demonstrate the application of Bayesian inference to a statistical model we present the following example.

*Example 2.1: Poisson likelihood and conjugate Gamma prior*

Consider the Poisson-Gamma statistical model $p(\lambda, Y_1, \ldots, Y_n)$ defined as
\begin{alignat}{2}
&\text{(\textit{Prior})}       &         \lambda &\sim \text{Gamma}(a, b), \\
&\text{(\textit{Likelihood})}  & \qquad      Y_i &\sim \text{Pois}(\lambda), \quad i = 1, \ldots, n.
\end{alignat}
The parameter of interest is the rate $\lambda > 0$, the data $Y = (Y_1, \ldots, Y_n)$ and $a, b > 0$ are fixed hyperparameters (parameters of the prior distribution). 
Having observed data $y_1, \ldots, y_n$, in terms of probability density functions the prior and likelihood are
\begin{alignat}{2}
&\text{(\textit{Prior})}      & \qquad            p(\lambda) &= \frac{b^a}{\Gamma(a)} \lambda^{a-1} e^{-b\lambda} \propto \lambda^{a-1} e^{-b\lambda}. \label{eq:gammaprior} \\
&\text{(\textit{Likelihood})} &         \mathcal{L}(\lambda) &= \prod_{i = 1}^n p(y_i \, | \, \lambda) = \prod_{i = 1}^n \frac{\lambda^{y_i} e^{-\lambda}}{\lambda!} \propto \lambda^{\sum_{i = 1}^n y_i} e^{-n\lambda}, \label{eq:poislike}
\end{alignat}
This particular prior has been chosen as it has functional form matching that of Equation \eqref{eq:poislike}, making it relatively simple to calculate the posterior distribution using Equation \eqref{eq:propbayes}, as follows
\begin{align}
p(\lambda \, | \, y_1, \ldots, y_n) &\propto \mathcal{L}(\lambda)p(\lambda) \nonumber \propto \lambda^{\sum_{i = 1}^n y_i} e^{-n\lambda} \cdot \lambda^{a-1} e^{-b\lambda} \nonumber \\
&= \lambda^{a - 1 + \sum_{i = 1}^n y_i} e^{-(b + n)\lambda} \propto \text{Gamma}(a + \sum_{i = 1}^n y_i, b + n). \label{eq:expost}
\end{align}
After updating conditional on the observed data, the posterior distribution \eqref{eq:expost} is of the same family, a Gamma distribution, as the prior distribution \eqref{eq:gammaprior}. 
This property is called conjugacy and allows exact Bayesian inference to be performed in some situations.

Having observed the data $\{1, 1, 2, 4, 1, 4\}$, Figure \eqref{fig:bayes} shows the posterior distribution contracting as beliefs about the rate parameter become stronger. \hfill $\square$

```{r dev = "tikz", echo=FALSE, fig.height = 3, fig.cap=paste("Particular example of Bayesian inference for the Poisson-Gamma model. The prior hyperparameters are $a = 3$ and $b = 1$. Data, plotted as open circles, is simulated from a Poisson distribution with $\\lambda = 2$ shown as the dashed line. \\label{fig:bayes}")}

x <- c(1, 1, 2, 4, 1, 4) # Simulated from lambda 2
y <- c(0, 0.05, 0, 0, 0.1, 0.05)

df <- data.frame(x, y)

ggplot(data = data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dgamma, n = 201, args = list(shape = 16, rate = 7),
                geom = "area", aes(fill = "1"), alpha = 0.4) +
  stat_function(fun = dgamma, n = 201, args = list(shape = 3, rate = 1),
                geom = "area", aes(fill = "2"), alpha = 0.4) +
  geom_point(data = df, aes(x = x, y = y), shape = 21) +
  scale_x_continuous(breaks = c(0, 5, 10)) +
  labs(x = "$\\lambda$", y = "$p(\\lambda)$") +
  geom_vline(xintercept = 2, colour="grey20", linetype = "longdash") +
  scale_fill_manual(values = c(darkblue, lightblue),
                    name = "",
                    labels = c("Posterior", "Prior")) +
  theme(panel.background = element_blank(),
        legend.justification = c(0, 0),
        legend.position = c(0.75, 0.6),
        axis.line = element_line(colour = "grey"))
```

## Graphical models \label{sec:graph}

\begin{figure}
\centering
\begin{tikzpicture}[
      > = stealth, % arrow head style
      shorten > = 1pt, % don't touch arrow head to node
      auto,
      node distance = 3cm, % distance between nodes
      semithick % line style
    ]

    \node[] at (0, 1.5) {(a)};
    \node[state] (1) at (0, 0) {$Z_1$};
    \node[state] (2) at (2, 0) {$Z_2$};
    \node[state] (3) at (4, 0) {$Z_3$};
    \node[state] (4) at (6, 0) {$Z_4$};

    \path (1) edge (2);
    \path (2) edge (3);
    \path (3) edge (4);
    \path (1) edge[bend right=30] node [left] {} (3);
    \path (1) edge[bend right=50] node [left] {} (4);
    \path (2) edge[out=40, in=140]  (4);

    \node[] at (8, 1.5) {(b)};
    \node[state] (5) at (9, 1) {$Z_1$};
    \node[state] (6) at (11, 1) {$Z_2$};
    \node[state] (7) at (13, 1) {$Z_3$};
    \node[state] (8) at (11, -1) {$Z_4$};

    \path (5) edge (8);
    \path (6) edge (8);
    \path (7) edge (8);
\end{tikzpicture}
\caption{Directed acyclic graphs for the four variables $Z_1, \ldots, Z_4$.}
\label{fig:dag1}
\end{figure}

The particular features of the joint distribution $p(\theta, Y)$ can be constructed in many ways. 
Note that typically both $\theta$ and $Y$ are multivariate, so specifying $p(\theta, Y)$ may be no small task.
Probabilistic graphical models (PGMs) [@bishop2006pattern] are one popular and useful tool for this purpose.

A graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ is defined by a set of vertices $\mathcal{V}$ together with a set of edges $\mathcal{E}$, where each edge connects a pair of vertices.
In a PGM, the vertices represent random variables and the edges represent probabilistic relationships between the random variables.
Bayesian networks are a particular type of PGM in which conditional dependencies (defined below) are the probabilistic relationships represented by the edges [@bishop2006pattern].

Consider random variables $Z_k \in \mathcal{Z}_k$, $k = 1, \ldots, K$ with joint distribution $p(Z_1, \ldots, Z_K)$.
$Z_k$ is conditionally independent from $Z_l$ given $Z_m$, written $Z_k \perp Z_l \, | \, Z_m$, if and only if
\begin{equation}
p(Z_k, Z_l \, | \, Z_k) = p(Z_k \, | \, Z_k)p(Z_l \, | \, Z_k) \label{eq:cindepdef}
\end{equation}
By repeated conditioning, it is always true that the joint distribution $p(Z_1, \ldots, Z_K)$ can be decomposed according to
\begin{align}
p(Z_1, \ldots, Z_K) &= p(Z_1)p(Z_2, \ldots, Z_K \, | \, Z_1) = p(Z_1) p(Z_2 \, | \, Z_1) p(Z_3, \ldots, Z_K \, | \, Z_1, Z_2) \nonumber \\
&= \cdots = p(Z_1) p(Z_2 \, | \, Z_1) \cdots p(Z_K \, | \, Z_1, \ldots, Z_{K-1}). \label{eq:dag1}
\end{align}
This factorisation can be represented by a particular kind of graph where the edges are directional, shown using arrows, and there are no directed cycles between the nodes.
Graphs like this are called directed acyclic graphs (DAGs).
A Bayesian network is a DAG where there is a directed edge from variable $Z_k$ to $Z_l$ if the conditional distribution of $Z_l$ depends on $Z_k$.
For example, the DAG corresponding to Equation \eqref{eq:dag1} with $K = 4$ is shown in part (a) of Figure \ref{fig:dag1}.

If Equation \eqref{eq:cindepdef} holds then conditional densities of the form $p(Z_k \, | \, Z_l, Z_k)$ can be rewritten to remove the dependence on $Z_l$ as follows
\begin{equation}
p(Z_k \, | \, Z_l, Z_k) = \frac{p(Z_k, Z_l \, | \, Z_k)}{p(Z_l \, | \, Z_k)} 
= \frac{p(Z_k \, | \, Z_k)p(Z_l \, | \, Z_k)}{p(Z_l \, | \, Z_k)}
= p(Z_k \, | \, Z_k). \label{eq:drop}
\end{equation}
Equation \eqref{eq:drop} shows that conditional independence statements allow some of the dependencies, and hence directed edges in DAGs, to be removed.
Suppose that $K = 4$ as before and each $Z_1, Z_2$ and $Z_3$ are conditionally independent from each other given $Z_4$.
Then, Equation \eqref{eq:dag1} may be rewritten as
\begin{equation}
p(Z_1, \ldots, Z_K) = p(Z_1)p(Z_2)p(Z_3)p(Z_4 \, | \, Z_1, Z_2, Z_3),  \label{eq:dag2}
\end{equation}
and the corresponding DAG redrawn taking into account this structure (part (b) of Figure \ref{fig:dag1}).

It is visually clear that conditional independence statements result in sparser graphs, thereby reducing model complexity.
More generally, given a set of conditional independence statements, joint probability distributions may be rewritten as
\begin{equation}
p(Z_1, \ldots, Z_K) = \prod_{k=1}^K p(Z_k \, | \, \text{pa}(Z_k)),
\end{equation}
where $Z_k$ is only conditionally dependent only on its parents $\text{pa}(Z_k)$.
In the corresponding DAG, there is a directed edge from each $Z_l \in \text{pa}(Z_k)$ to $Z_k$.
For example, in Figure \ref{fig:dag1}, $\text{pa}(Z_4) = \{Z_1, Z_2, Z_3\}$.

Bayesian networks allow complex models $p(Z_1, \ldots, Z_k)$ to be built up from the simpler building blocks of $p(Z_k \, | \, \text{pa}(Z_k))$. 
In the context of Bayesian statistics, supposing the prior parameter is multivariate $\theta = (\theta_1, \ldots, \theta_K)$ then the prior distribution may be decomposed according to $\prod_{k=1}^K p(\theta_k \, | \, \text{pa}(\theta_k))$. 
The statistical model $p(\theta, Y)$ may then be written as a product of the prior with the likelihood $p(Y \, | \, \text{pa}(Y))$.
Bayesian models which are built up in stages this way are known as hierarchical models.

These models possess the flexibility required to model the varied data sources necessary for evidence synthesis.
Furthermore, domain experts can often efficaciously express their beliefs via Bayesian networks due to their ease of interpretability.

## Modular models

Having introduced the necessary prerequisites, we may now discuss how they might be used to synthesise evidence.

Consider multiple sources of observable data $Y_m \in \mathcal{Y}_m$ with $m = 1, \ldots M$. 
Each $Y_m$ is, at least to some extent, directly informative about a shared parameter $\phi$ which is common to all of the models.
Therefore, for each evidence source, define a statistical model $p_m(\phi, \psi_m, Y_m)$, which will be henceforth called a submodel.
For each submodel the parameters are $(\phi, \psi_m)$: the link parameter $\phi \in \Phi$ together with model specific parameters  $\psi_m \in \Psi_m$ unique to each submodel.

It is essential that each of the submodels agree conceptually on what the link parameter $\phi$ represents: conversations in which the participants do not agree on the basic tenants are rarely productive.
It is worth mentioning this fact because in some models the interpretation of parameters can be subtle.
For example, in a regression the regression coefficient of a covariate implicitly takes into account the inclusion and exclusion of all other potential covariates; thereby making it a different parameter in different models and rarely an appropriate link parameter.
It is therefore safest if $\phi$ has a clear interpretation linked to the real world.

Given the collection of submodels $p_m(\phi, \psi_m, Y_m)$, the aim of the joining operation in Markov melding [@goudie2019joining] is to produce a joint distribution $p(\phi, \psi_1, \ldots, \psi_M, Y_1, \ldots, Y_M)$ over all parameters and observable data.
Synthesising a joint distribution is attractive as it facilitates the propagation of uncertainty between the submodels.
Additionally, the joint distribution defines a statistical model to which general methodological tools can be applied.
Higher quality inference about $\phi$ is typically the primary aim of evidence synthesis, although due to the sharing of information between the submodels it may be that inference for $\psi_m$ $m = 1, \ldots, M$ is also improved.

The joining of submodels is best suited to situations where the model specific observed quantities $Y_m$ are relatively distinct. 
The more unique information there is dispersed across the different submodels, the greater the potential benefits of evidence synthesis.
On the other extreme, the observed data $Y_m = Y$ may be identical for each of the $M$ submodels (with different likelihood components say).
This is in some sense unappealing as each of the submodels then offers a competing explanation for the data generating mechanism.
Such a collection of beliefs cannot be reasonably held confidently by a single individual simultaneously.
Rather, the individual may have to assign prior probabilities to each of the models being true.
Then, if the link parameter plays different roles in the different submodels, although Markov melding may be possible it might not be advisable.
In machine learning, multiple models may be trained on the same data and aggregated for the purposes of prediction [@bishop2006pattern].
However, in this setting the focus is not on model building, interpretation or uncertainty quantification.

Markov melding extends and unifies previous work, particularly Markov combination [@dawid1993hyper] and Bayesian melding [@poole2000inference] which we review here. 
In particular, Markov combination considers joining submodels where an added simplifying assumption is made; whilst Bayesian melding provides the methodological inspiration for overcoming this assumption.

## Markov combination

As above consider the collection of submodels $p_m(\phi, \psi_m, Y_m)$, $m = 1, \ldots, M$.
Each submodel features a prior distribution on the link parameter $\phi$ which in the simplest case is specified directly with $p_m(\phi, \psi_m, Y_m) = p_m(\psi_m, Y_m \, | \, \phi)p_m(\phi)$ such that $\text{pa}(\phi) = \emptyset$. 
Alternatively, the prior can be accessed by marginalising out both $\psi_m$ and $Y_m$ as follows
\begin{equation}
p_m(\phi) = \iint p_m(\phi, \psi_m, Y_m) d\psi_m dY_m. \label{eq:priormarginal}
\end{equation}

An important property of the prior distributions is consistency; where the marginals $p_m(\phi)$ of the link parameter $\phi$ are called consistent if
\begin{equation}
\forall m \; p_m(\phi) = p(\phi). \label{eq:consistent}
\end{equation}
If the prior marginals are consistent then the submodels each have the same beliefs about $\phi$ in advance of seeing the data.
As a result, all of the submodels and their associated beliefs could theoretically reasonably be held by a single individual without any self-contradiction.
(That being said, in practice a single practitioner may justifiably place different priors on the same parameter depending on the particular likelihood component.
This might be the case if there exists a conjugate prior, as in Example 2.1, for example.)

Supposing that assumption \eqref{eq:consistent} holds, @dawid1993hyper define a joint model $p_{\mathrm{comb}}(\phi, \psi_1, \ldots, \psi_M, Y_1, \ldots, Y_M)$ called the Markov combination of the submodels $p_1, \ldots, p_M$.
In advance of justification, this joint model $p_{\mathrm{comb}}$ is
\begin{equation}
p_{\mathrm{comb}}(\phi, \psi_1, \ldots, \psi_M, Y_1, \ldots, Y_M) 
= \frac{\prod_{m=1}^{M} p_m(\phi, \psi_m, Y_m)}{p(\phi)^{M-1}}. \label{eq:comb}
\end{equation}
The construction of $p_{\mathrm{comb}}$ is based on prior consistency together with an additional assumption. 
This assumption is that, conditional on the link parameter, the models are independent.
To be exact
\begin{equation}
\forall m \neq \ell \; (\psi_m, Y_m) \perp (\psi_\ell, Y_\ell) \, | \, \phi, \label{eq:cindep} 
\end{equation}
where conditional independence is defined as in Equation \eqref{eq:cindepdef}.
Figure \eqref{fig:cindepdag} illustrates this assumption using a DAG for $M = 2$ submodels.

In some sense, the truth of this assumption justifies the use of seperate submodels, rather than a single monolithic model which is specified at the outset.
Just as with DAGs, conditional independence statements simplify the process of modelling: it is easier to specify a collection of submodels than it is to additionally consider their interactions.
This is particularly pertinent if the observable data $Y_m$ are from different background areas.
Here, the modelling process naturally decomposes such that different domain experts are consultated for each data source, resuling in submodel specifications.
In this situation, the conditional independence assumption also seems, in general, quite reasonable.

\begin{figure}
\centering
\begin{tikzpicture}[
      > = stealth, % arrow head style
      shorten > = 1pt, % don't touch arrow head to node
      auto,
      node distance = 3cm, % distance between nodes
      semithick % line style
    ]

    \node[state] (1) at (0,0){$\psi_1$};
    \node[state] (2) at (1,-2) {$Y_1$};
    \node[state] (3) at (2,0) {$\phi$};
    \node[state] (4) at (3,-2) {$Y_2$};
    \node[state] (5) at (4,0){$\psi_2$};

    \path (1) edge  (2);
    \path (3) edge  (2);
    \path (3) edge  (4);
    \path (5) edge (4);
\end{tikzpicture}
\caption{Directed acyclic graph corresponding to Equation \eqref{eq:cindep} for $M = 2$.}
\label{fig:cindepdag}
\end{figure}

The structure of Equation \eqref{eq:comb} is then explained by applying these two assumptions to a hypothetical joint model
\begin{align}
p(\phi, \psi_1, \ldots, \psi_M, Y_1, \ldots, Y_M)
&\stackrel{\phantom{\eqref{eq:cindep}}}{=} p(\phi) \, p(\psi_1, \ldots, \psi_M, Y_1, \ldots, Y_M \, | \, \phi) \nonumber \\
&\stackrel{\eqref{eq:cindep}}{=} p(\phi) \prod_{m=1}^{M} p_m(\psi_m, Y_m \, | \, \phi) \nonumber \\
&\stackrel{\phantom{\eqref{eq:cindep}}}{=} p(\phi) \prod_{m=1}^M \frac{p_m(\phi, \psi_m, Y_m)}{p_m(\phi)} \label{eq:premeld} \\
&\stackrel{\eqref{eq:consistent}}{=} \frac{\prod_{m=1}^{M} p_m(\phi, \psi_m, Y_m)}{p(\phi)^{M-1}},
\end{align}
where the result is exactly that of Equation \eqref{eq:comb}.

Markov combination has the attractive property that submodel marginals and submodel-specific conditional distributions are preserved, that is $p_{\mathrm{comb}}(\phi, \psi_m, Y_m) = p_m(\phi, \psi_m, Y_m)$ and $p_{\mathrm{comb}}(\psi_m, Y_m \, | \, \phi) = p_m(\psi_m, Y_m \, | \, \phi)$ for all $m$ [@goudie2019joining].
This can be shown via
\begin{align}
p_{\mathrm{comb}}(\phi, \psi_m, Y_m) &= \int p_{\mathrm{comb}}(\phi, \psi_1, \ldots, \psi_M, Y_1, \ldots, Y_M) d\psi_{-m} dY_{-m} \nonumber \\
&= \int p(\phi) \prod_{m=1}^{M} p_m(\psi_m, Y_m \, | \, \phi) d\psi_{-m} dY_{-m} \nonumber \\
&= p(\phi) p_m(\psi_m, Y_m \, | \, \phi) = p_m(\phi, \psi_m, Y_m),
\end{align}
where the notation $d\psi_{-m} dY_{-m}$ refers to integration with respect to each of the model specific parameters and observed quantites except from $\psi_m$ and $Y_m$. 
Similarly
\begin{align}
p_{\mathrm{comb}}(\psi_m, Y_m \, | \, \phi) &= \frac{p_{\mathrm{comb}}(\phi, \psi_m, Y_m)}{p(\phi)} = \frac{p(\phi, \psi_m, Y_m)}{p(\phi)} = p_m(\psi_m, Y_m \, | \, \phi).
\end{align}

In its form, the Markov melded joint model is similar to that of Markov combination.
This is because both methods assume the conditional independence assumption, given by Equation \eqref{eq:cindep}.
The primary difference is that Markov melding does not assume that the prior marginals of the link parameter are consistent.
Instead, these marginals are ``melded'' by a process similar to that developed in Bayesian melding, which we will now detail.

## Bayesian melding \label{sec:bayesianmelding}

The presence of multiple priors on a given quantity also arises in the study of simulation models.
Bayesian melding [@poole2000inference] is motivated by this challenge and aims to perform inference on deterministic simulation models $M: \theta \to \phi$.
The outputs $\phi \in \Phi \subseteq \mathbb{R}^p$ are a function of the inputs $\theta \in \Theta \subseteq \mathbb{R}^n$ only, such that $\phi = M(\theta)$.
There may exist data relating to one or both of $\theta$ and $\phi$ which we denote by $Y_\theta$ and $Y_\phi$ respectively.
Irrespective of the simulation model $M$, a statistical model $p(\theta, \phi, Y_\theta, Y_\phi)$ can be specified which @poole2000inference additionally assume can be decomposed into a product of independent prior and likelihood components
\begin{equation}
p(\theta, \phi, Y_\theta, Y_\phi) = p(Y_\theta \, | \, \theta)p(\theta)p(Y_\phi \, | \, \phi)p(\phi).
\end{equation}
We call this distribution the joint premodel prior distribution.
It may be marginalised to find the premodel prior distributions $p(\theta)$ and $p(\phi)$ respectively.

Bayesian melding builds upon a previous approach called Bayesian synthesis [@raftery1995inference]. 
In this approach, the model $M$ is incorporated into the joint premodel distribution to create a joint postmodel distribution $\pi(\theta, \phi, Y_\theta, Y_\phi)$.
@poole2000inference describe that this is done by restricting $p(\theta, \phi, Y_\theta, Y_\phi)$ to the submanifold $\{(\theta, \phi, Y_\theta, Y_\phi):\phi = M(\theta)\}$, such that
\begin{equation}
\pi(\theta, \phi, Y_\theta, Y_\phi) \propto
\begin{cases}
  p(\theta, M(\theta), Y_\theta, Y_\phi), & \text{if } \phi = M(\theta) \\
  0, & \text{otherwise.}
\end{cases}
\end{equation}
The marginal postmodel prior distribution for $\theta$ is then
\begin{equation}
\pi(\theta) = p(\theta \, | \, \phi = M(\theta)) \label{eq:wolpert}
\end{equation}
However, in a discussion of the paper, @wolpert1995inference call attention to the fact that conditional distributions of the form \eqref{eq:wolpert} are ill-defined and therefore subject to what is known as the Borel-Kolmogorov paradox. 
A key repercussion of this fact is that the postmodel distribution depends on how $M$ is parametrised.
Furthermore, @schweder1996bayesian show that any postmodel distribution can be obtained by arbitrary parametrisation of $M$.
These observations place serious concern on the usefulness of Bayesian synthesis, motivating the development of Bayesian melding.

The problem that @wolpert1995inference find arises because in Bayesian synthesis, as well as the premodel marginal prior distribution $p(\phi)$, there is an additional prior on the output $\phi$.
In particular, applying the deterministic model $M$ to the premodel marginal prior distribution on inputs $p(\theta)$ results in a model-induced prior on outputs $\phi$ denoted by $p^\star(\phi)$. 
If $M^{-1}$ exists then this prior is given by
\begin{equation}
p^\star(\phi) = p(M^{-1}(\phi)) |J(\phi)|, \label{eq:transform}
\end{equation}
where $J(\phi)$ is the Jacobian of the transformation.
The two priors are typically inconsistent as they are based on different information.

A simple approach to rectifying this problem would be not to attempt to place a separate prior on outputs $\phi$, instead relying on the prior induced by applying $M$ to $p(\theta)$
However, in hopes of combining both information from the premodel prior and that of the model induced prior, Bayesian melding instead replaces $p(\phi)$ and $p^\star(\phi)$ by a single melded output prior $\tilde p(\phi)$.
For their particular motivating application, relating to a population dynamics model for bowhead whales, @poole2000inference do this by taking the (normalised) geometric mean of the two priors as follows
\begin{equation}
\tilde p(\phi) \propto p(\phi)^{0.5}p^\star(\phi)^{0.5} \label{eq:geometric}
\end{equation}
Equation \eqref{eq:geometric} is an example of a broader class of pooling operations which mathematically combine probability distributions.
More examples will be discussed further in the next section.

@poole2000inference propose then inverting the prior $\tilde p(\phi)$ to the input space to define a melded input prior $\tilde p(\theta)$ as (omitting some details)
\begin{align}
\tilde p(\theta) &= \tilde p(M(\theta)) \left(\frac{p(\theta)}{p^\star(M(\theta))}\right) \nonumber \\
&\propto p(\theta) \left(\frac{p(M(\theta))}{p^\star(M(\theta))}\right)^{1-\alpha}. \label{eq:inversion}
\end{align}
Equation \eqref{eq:inversion} corresponds to the original input prior $p(\theta)$ weighted, according to $\alpha$, by a ratio of the output prior and the model induced prior evaluated at $\phi = M(\theta)$ for given value of $\theta \in \Theta$.

To conclude discussion of Bayesian melding, having observed $Y_\theta = y_\theta$ and $Y_\phi = y_\phi$ a standard Bayesian posterior for $\theta$ can be then defined as follows
\begin{equation}
\pi(\theta \, | \, y_\phi, y_\theta) \propto p(y_\theta \, | \, \theta)p(y_\phi \, | \, M(\theta)) \tilde p(\theta),
\end{equation}
allowing inference to proceed as usual.

## Combining expert opinion \label{sec:experts}

The prior pooling step, for which Equation \eqref{eq:geometric} is one instance, in Bayesian melding has methodological similarities with previous work on combining the opinions of multiple experts [@o2006uncertain].
Most relevant from this literature is mathematical aggregation, where distributions are elicited independently from each expert individually and then combined by some mathematical rule.
The primary alternative to mathematical aggregation is behavioural aggregation, where the group of experts interact and a single distribution is elicited from the group after discussion.
In the case of Markov melding, in general the practitioners who originally developed each of the submodels may not be willing and able to justify their choices as would be required in behavioural aggregation.

Just as it is important that the submodels in Markov melding have a shared concept of the link parameter, @clemen1999combining caution that "the mathematical and behavioural approaches ... assume that the experts have ironed out differences in definitions and that they agree on exactly what is to be forecast or assessed'';
also noting that ``practising risk analysts know these are strong assumptions".

In order to outline some of the proposed [@o2006uncertain] approaches, suppose each of a group of $n$ experts are independently asked their beliefs about an unknown quantity $\theta$, resulting in elicited distributions $f_i(\theta), i = 1, \ldots, n$. 
The simplest and most widely used technique within mathematical aggregation is opinion pooling, where a consensus distribution $f(\theta)$ is obtained as some function of the individual distributions $\{f_1(\theta), \ldots, f_n(\theta)\}$.
Two of the most common examples are the linear opinion pool \eqref{eq:linpool} and logarithmic opinion pool \eqref{eq:logpool} defined respectively as
\begin{align}
f(\theta) \propto \sum_{i=1}^{n} w_i f_i(\theta) \label{eq:linpool}, \\ 
f(\theta) \propto \prod_{i=1}^{n} f_i(\theta)^{w_i} \label{eq:logpool},
\end{align}
where $f(\theta)$ is normalised to a probability density function. The weights $w_i$ can be chosen freely, either giving more weight to some experts than others or choosing to weight all of the experts equally. 

Dictatorial pooling, in which one experts opinion is chosen as the consensus distribution, is special case of linear opinion pooling.
To see this, set $w_j = 0$ for all $j \neq i$ in Equation \eqref{eq:linpool} such that $f(\theta) = f_i(\theta)$.
In Section \ref{sec:bayesianmelding}, the suggestion to set $\tilde p(\phi) = p^\star(\phi)$ is an example of dictatorial pooling.
The method used by @poole2000inference, geometric pooling given by Equation \eqref{eq:geometric}, is an example of logarithmic pooling with $w_i = 1/M$. 
Another special case of logarithmic opinion pooling is the product of experts pooling [@hinton2002training] which sets $w_i = 1$ for all $i$ such that
\begin{equation}
f(\theta) \propto \prod_{i=1}^{n} f_i(\theta). \label{eq:poe}
\end{equation}

```{r dev = "tikz", echo=FALSE, fig.height = 7, fig.cap=paste("Opinion pooling of solid line $\\mathcal{N}(1, 1)$ with dashed line Gaussians, specified by plot text. Dashed line Gaussians are weighted $w_1$ with $w_2 = 1 - w_1$. Reproduced with alternations from @goudie2019joining. \\label{fig:pooling}")}
# Reproduce Figure 4 from Melding paper (probably very inefficiently)

p <- c(lightblue, lightpink, lightgold)

dlin_pool <- function(x, mean, sd, w, k) { # Linear pool two densities, equal weighting
  k * (w[1] * dnorm(x, mean[1], sd[1]) + w[2] * dnorm(x, mean[2], sd[2]))
}

dlog_pool <- function(x, mean, sd, w, k) { # Logarithmic pool two densities, equal weighting
  k * dnorm(x, mean[1], sd[1])^w[1] * dnorm(x, mean[2], sd[2])^w[2]
}

dPoE_pool <- function(x, mean, sd, k) { # PoE pool two densities, equal weighting
  k * dnorm(x, mean[1], sd[1]) * dnorm(x, mean[2], sd[2])
}

pool_plot <- function(mean, sd, w, my_sub) {
  k1 <- 1/integrate(dPoE_pool, mean, sd,
                    k = 1, lower = -25, upper = 25)$value
  k2 <- 1/integrate(dlog_pool, mean, sd,
                    w = w, k = 1, lower = -25, upper = 25)$value
  k3 <- 1/integrate(dlin_pool, mean, sd,
                    w = w, k = 1, lower = -25, upper = 25)$value
  ggplot(data = data.frame(x = c(-6, 6)), aes(x)) +
    stat_function(fun = dnorm, n = 201,
                  args = list(mean = mean[1], sd = sd[1]),
                  lty = "dashed", lwd = .3) +
    stat_function(fun = dnorm, n = 201,
                  args = list(mean = mean[2], sd = sd[2]),
                  lwd = .3) +
    stat_function(fun = dPoE_pool, n = 201, args = list(mean, sd, k1),
                  geom = "area", aes(fill = "1"), alpha = 0.5) +
    stat_function(fun = dlog_pool, n = 201, args = list(mean, sd, w, k2),
                  geom = "area", aes(fill = "2"), alpha = 0.5) +
    stat_function(fun = dlin_pool, n = 201, args = list(mean, sd, w, k3),
                  geom = "area", aes(fill = "3"), alpha = 0.5) +
    labs(subtitle = my_sub) +
    scale_fill_manual(values = c(lightpink, lightgold, lightblue),
                      name = "",
                      labels = c("Product of Experts", "Logarithmic Opinion Pool", "Linear Opinion Pool"),
                      guide = FALSE) +
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          panel.background = element_blank(),
          axis.line = element_line(colour = "grey"))
}

p1 <- pool_plot(mean = c(1, 1), sd = c(2, 1), w = c(0.75, 0.25), my_sub = "(a) $\\mathcal{N}(1, 4), w_1 = 0.75$")
p2 <- pool_plot(mean = c(1, 1), sd = c(2, 1), w = c(0.5, 0.5), my_sub = "(b) $\\mathcal{N}(1, 4), w_1 = 0.5$")
p3 <- pool_plot(mean = c(1, 1), sd = c(2, 1), w = c(0.25, 0.75), my_sub = "(c) $\\mathcal{N}(1, 4), w_1 = 0.25$")

p4 <- pool_plot(mean = c(-1, 1), sd = c(2, 1), w = c(0.75, 0.25), my_sub = "(d) $\\mathcal{N}(-1, 4), w_1 = 0.75$")
p5 <- pool_plot(mean = c(-1, 1), sd = c(2, 1), w = c(0.5, 0.5), my_sub = "(e) $\\mathcal{N}(-1, 4), w_1 = 0.5$")
p6 <- pool_plot(mean = c(-1, 1), sd = c(2, 1), w = c(0.25, 0.75), my_sub = "(f) $\\mathcal{N}(-1, 4), w_1 = 0.25$")

p7 <- pool_plot(mean = c(-3, 1), sd = c(1, 1), w = c(0.75, 0.25), my_sub = "(g) $\\mathcal{N}(-3, 1), w_1 = 0.75$")
p8 <- pool_plot(mean = c(-3, 1), sd = c(1, 1), w = c(0.5, 0.5), my_sub = "(h) $\\mathcal{N}(-3, 1), w_1 = 0.5$")
p9 <- pool_plot(mean = c(-3, 1), sd = c(1, 1), w = c(0.25, 0.75), my_sub = "(i) $\\mathcal{N}(-3, 1), w_1 = 0.25$")

legend <- cowplot::get_legend(
  p1 + 
    guides(fill = guide_legend(nrow = 1)) +
    theme(legend.position = "bottom")
)

grid <- cowplot::plot_grid(p1, p2, p3, p4, p5, p6, p7, p8, p9, ncol = 3)

cowplot::plot_grid(grid, legend, ncol = 1, rel_heights = c(1, 0.2))
```

Figure \eqref{fig:pooling} shows the Product of Experts, logarithmic and linear opinion pooling applied to various Gaussian distributions.
Note that in the Product of Experts pooling, there are no weights. 
Or, another way of looking at it, the weights are fixed to be $w_1 = w_2 = 1$.

The Product of Experts is named as such because it gives each expert the benefit of the doubt in assuming that their beliefs are correct.
The result, as most clearly illustrated by row (g, h, i) of Figure \eqref{fig:pooling}, is that the regions where the experts overlap has to be the truth.
Logarithmic pooling is slightly more doubtful of the experts but still produces relatively contracted pooled beliefs in comparison to the more cautious linear pooling.
@o2006uncertain find that "while the linear opinion pool has been quite widely used in practice, the logarithmic opinion pool has been largely ignored, perhaps because it is perceived to lead to unrealistically strong aggregated beliefs".

Row (a, b, c) of Figure \eqref{fig:pooling} prompt another consideration: to what extend should expert agreement lead to higher confidence.
To use an analogy, suppose you and a group of friends want to decide whether or not to eat at a certain restaurant. 
If each friend expresses a positive opinion about the restaurant then may seem reasonable to be more confident than any given friend about the restaurant.
However, if each friend bases their opinion solely on a positive review of the restaurant in that week's paper then this effectively would be double counting; in reality there is only one ``expert'' - the journalist.

## Markov melding

If the marginals are inconsistent, that is condition \eqref{eq:consistent} fails to hold, then the approach of Markov combination requires some modification.
Just as pooling is used to reconcile disagreement between the premodel prior and induced prior in Bayesian melding, the inconsistent prior marginals $p_m(\phi)$ can be also pooled to allow Bayesian inference to proceed.
The resulting pooled prior $p_\text{pool}(\phi)$ is such that
\begin{equation}
p_\text{pool}(\phi) = g(p_1(\phi), \ldots, p_M(\phi)), \label{eq:pooledprior}
\end{equation}
for some choice of pooling function $g$.
As discussed in Section \ref{sec:experts}, many possible pooling functions are possible and could be reasonably justified depending on the situation.
That being said, some choices of pooling function may be more computational efficient and so attractive from a practical point of view.

The Markov melded joint model $p_{\text{meld}}$ can be achieved by replacing $p(\theta)$ in Equation \eqref{eq:premeld} with the pooled prior \eqref{eq:pooledprior} such that
\begin{equation}
p_{\text{meld}}(\phi, \psi_1, \ldots, \psi_M, Y_1, \ldots, Y_M) = p_\text{pool}(\phi) \prod_{m=1}^M \frac{p_m(\phi, \psi_m, Y_m)}{p_m(\phi)} \label{eq:meld}
\end{equation}

This construction is equivalent to altering each of the submodels by a procedure @goudie2019joining call marginal replacement and then applying Markov combination. 
In particular, the marginal $p_m(\phi)$ of the original submodel is replaced by $p_\text{pool}(\phi)$ resulting in the replacement submodel
\begin{equation}
p_{\text{repl,m}}(\phi, \psi_m, Y_m) = p_m(\psi_m, Y_m \, | \, \phi)p_\text{pool}(\phi).
\end{equation}
After marginal replacement has occurred it is possible to apply Markov combination to the collection of replacement submodels $\{p_{\text{repl,m}}\}_{m = 1,\ldots,M}$ since they have consistent marginals, such that
\begin{align}
p_{\mathrm{comb}}(\phi, \psi_1, \ldots, \psi_M, Y_1, \ldots, Y_M) 
&= \frac{\prod_{m=1}^{M} p_{\text{repl,m}}(\phi, \psi_m, Y_m)}{p_\text{pool}(\phi)^{M-1}} \nonumber \\
&= \frac{\prod_{m=1}^{M} p_m(\psi_m, Y_m \, | \, \phi)p_\text{pool}(\phi)}{p_\text{pool}(\phi)^{M-1}} \nonumber \\
&= p_\text{pool}(\phi) \prod_{m=1}^{M} p_m(\psi_m, Y_m \, | \, \phi) \nonumber \\
&= p_\text{pool}(\phi) \prod_{m=1}^{M} \frac{p_m(\psi_m, Y_m, \phi)}{p_m(\phi)}.
\end{align}
The result is equal to that of the Markov melded joint model, Equation \eqref{eq:meld}.

Since a joint model has been obtained, Bayesian inference can proceed as usual.
Given observed data $Y_m = y_m$ for $m = 1, \ldots, M$, under the Markov melded model \eqref{eq:meld} the joint posterior distribution is
\begin{align}
p_{\text{meld}}(\phi, \psi_1, \ldots, \psi_M \, | \, y_1, \ldots, y_M) \propto p_{\text{pool}}(\phi) \prod_{m=1}^{M} \frac{p_m(\phi, \psi_m, y_m)}{p_m(\phi)}, \label{eq:meldpost}
\end{align}
which is called the Markov melded posterior.

We present the following pedagogical example of Markov melding with $M = 2$ submodels which, as with Example 3.1, uses conjugacy to enable exact Bayesian computation.

*Example 2.2: Binomial and Geometric submodels with conjugate Beta priors*

\begin{figure}
\centering
\begin{tikzpicture}[
      > = stealth, % arrow head style
      shorten > = 1pt, % don't touch arrow head to node
      auto,
      node distance = 3cm, % distance between nodes
      semithick % line style
    ]

    \node[] at (0, 1) {(a) Submodel 1: $p_1(\theta, Y_1)$};
    \node[constant] (1) at (0,0){$a$};
    \node[constant] (2) at (2,0) {$b$};
    \node[state] (3) at (1,-2) {$\theta$};
    \node[constant] (4) at (3,-2) {$m$};
    \node[state] (5) at (2,-4){$Y_1^i$};
    \node[above right] at (0, -5) {$i = 1, \ldots, n_1$}; 

    \path (1) edge  (3);
    \path (2) edge  (3);
    \path (3) edge  (5);
    \path (4) edge (5);
    
    \draw (0,-3) -- (3,-3) -- (3,-5) -- (0,-5) -- cycle;

    \node[] at (7, 1) {(b) Submodel 2: $p_2(\theta, Y_2)$};
    \node[constant] (6) at (7,0){$c$};
    \node[constant] (7) at (9,0) {$d$};
    \node[state] (8) at (8,-2) {$\theta$};
    \node[state] (9) at (8,-4) {$Y_2^i$};
    \node[above right] at (6.5,-5) {$i = 1, \ldots, n_2$}; 

    \path (6) edge  (8);
    \path (7) edge  (8);
    \path (8) edge  (9);
    
    \draw (6.5,-3) -- (9.5,-3) -- (9.5,-5) -- (6.5,-5) -- cycle;
    
\end{tikzpicture}
\caption{Further to the notation introduced in Section \ref{sec:graph}, constants may be included in DAGs with square boxes, whereas random variables have circular boxes. Rectangular boxes (sometimes called plates) are drawn around sections of the diagram which are repeated across the indices indicated.}
\label{fig:ex1dag}
\end{figure}

Consider the following two submodels, illustrated as DAGs in Figure \ref{fig:ex1dag}.

**Submodel 1** Define the first submodel $p_1(\theta, Y_1)$ as
\begin{align}
\theta &\sim \text{Beta}(a, b), \label{eq:prior1} \\
Y_1^i &\sim \text{Bin}(m, \theta), \quad i = 1, \ldots, n_1,
\end{align}
where $a = 2$, $b = 3$ and $m = 10$ are known and $Y_1 = (Y_1^1, \ldots, Y_1^{n_1})$.

Suppose the true value of $\theta = 0.4$. 
We simulate observed data $Y_1 = y_1$ and of size $n_1 = 3$ from this submodel, resulting in $y_1 = \{3, 5, 4\}$.
Conditioning on the observed data, inference can be done exactly as the prior is conjugate.
The posterior distribution is given by
\begin{align}
p_1(\theta \, | \, y_1) &\propto p_1(y_1 \, | \, \theta) p_1(\theta) 
= \prod_{i=1}^{n_1} {m\choose y_1^i} \theta^{y_1^i}  (1- \theta)^{m-{y_1^i}} \cdot 
   \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} \theta^{a-1} (1- \theta)^{b-1} \nonumber \\
&\propto \theta^{\tau_1 + a - 1}  (1- \theta)^{n_1m-\tau_1 + b - 1} 
\propto \text{Beta}(\tau_1 + a, n_1m-\tau_1 + b),
\end{align}
where $\sum_{i=1}^{n_1} y_1^i = \tau_1$.

**Submodel 2** The second submodel $p_2(\theta, Y_2)$ is such that
\begin{align}
\theta &\sim \text{Beta}(c, d) \label{eq:prior2} \\
Y_2 &\sim \text{Geo}(\theta), \quad i = 1, \ldots, n_2,
\end{align}
where $c = 5$ and $d = 4$ are known and $Y_1 = (Y_2^1, \ldots, Y_2^{n_2})$.

We again simulate observed data $Y_2 = y_2$ this time of size $n_2 = 6$, resulting in $y_2 = \{2, 0, 1, 0, 0, 1\}$.
This submodel, like the one before, is conjugate and the posterior distribution is simply
\begin{align}
p_2(\theta \, | \, y_2) &\propto p_2(y_2 \, | \, \theta) p_2(\theta)
= \prod_{i=1}^{n_2} (1-\theta)^{y_2^i} \theta \cdot 
   \frac{\Gamma(c)\Gamma(d)}{\Gamma(c+d)} \theta^{c-1} (1- \theta)^{d-1} \nonumber \\
&\propto \theta^{n_2 + c - 1}  (1- \theta)^{\tau_2 + d - 1}
\propto \text{Beta}(n_2 + c, \tau_2 + d),
\end{align}
where $\sum_{i=1}^{n_2} y_2^i = \tau_2$.

```{r echo=FALSE}
set.seed(3)
theta <- 0.4
n1 <- 3; n2 <- 6
m <- 10
a <- 2; b <- 3
c <- 5; d <- 4
y1 <- rbinom(n1, m, theta)
tau1 <- sum(y1)
y2 <- rgeom(n2, theta)
tau2 <- sum(y2)
```

```{r dev = "tikz", echo=FALSE, fig.height = 5.5, fig.cap=paste("Parts (a) and (b) show the standard Bayesian updating of the prior for submodels one and two respectively. Part (c) shows the PoE pooled prior and Markov melded posterior. \\label{fig:ex1}")}
p1 <- ggplot(data = data.frame(x = c(0, 1)), aes(x)) +
  stat_function(fun = dbeta, n = 100, 
                args = list(shape1 = a + tau1, shape2 = b + n1*m - tau1),
                geom = "area", aes(fill = "1"), alpha = 0.6) +
  stat_function(fun = dbeta, n = 100, 
                args = list(shape1 = a, shape2 = b),
                geom = "area", aes(fill = "2"), alpha = 0.6) +
  geom_vline(xintercept = theta, colour="grey20", linetype = "longdash") +
  ylim(c(0, 6.5)) +
  scale_fill_manual(values = c(darkblue, lightblue),
                    name = "",
                    labels = c("Posterior", "Prior")) +
  labs(subtitle = "(a) Submodel 1", x = "$\\theta$", y = "$p(\\theta)$") +
  theme(panel.background = element_blank(),
        legend.justification = c(0, 0),
        legend.position = c(0.7, 0.6),
        axis.line = element_line(colour = "grey"))


p2 <- ggplot(data = data.frame(x = c(0, 1)), aes(x)) +
  stat_function(fun = dbeta, n = 100, 
                args = list(shape1 = n2 + c, shape2 = tau2 + d),
                geom = "area", aes(fill = "1"), alpha = 0.6) +
  stat_function(fun = dbeta, n = 100, 
                args = list(shape1 = c, shape2 = d),
                geom = "area", aes(fill = "2"), alpha = 0.6) +
  geom_vline(xintercept = theta, colour="grey20", linetype = "longdash") +
  ylim(c(0, 6.5)) +
  scale_fill_manual(values = c(darkgreen, lightgreen),
                    name = "",
                    labels = c("Posterior", "Prior")) +
  labs(subtitle = "(b) Submodel 2", x = "$\\theta$", y = "$p(\\theta)$") +
  theme(panel.background = element_blank(),
        legend.justification = c(0, 0),
        legend.position = c(0.7, 0.6),
        axis.line = element_line(colour = "grey"))

p3 <- ggplot(data = data.frame(x = c(0, 1)), aes(x)) +
  stat_function(fun = dbeta, n = 100, 
                args = list(shape1 = tau1 + a + n2 + c - 1, shape2 = n1*m - tau1 + b + tau2 + d - 1),
                geom = "area", aes(fill = "1"), alpha = 0.6) +
  stat_function(fun = dbeta, n = 100, 
                args = list(shape1 = a + c - 1, shape2 = b + d - 1),
                geom = "area", aes(fill = "2"), alpha = 0.6) +
    geom_vline(xintercept = theta, colour="grey20", linetype = "longdash") +
    labs(subtitle = "(c) Melded model", x = "$\\theta$", y = "$p(\\theta)$") +
    scale_fill_manual(values = c(darkpink, lightpink),
                      name = "",
                      labels = c("Posterior", "Prior")) +
    theme(panel.background = element_blank(),
          legend.justification = c(0, 0),
          legend.position = c(0.75, 0.6),
          axis.line = element_line(colour = "grey"))

p4 <- cowplot::plot_grid(p1, p2, ncol = 2)

cowplot::plot_grid(p4, p3, ncol = 1)
```

**Melded model**
This is a simple situation: the link parameter is $\phi = \theta$, and there are no model specific parameters $\psi_1 = \psi_2 = \emptyset$.
Notice with this hyperparameter setting, there is not consistency between the two Beta priors, given by Equations \eqref{eq:prior1} and \eqref{eq:prior2}, on the link parameter. 
For this reason, Markov melding is an appropriate methodological choice to perform inference.

To facilitate Markov melding, we pool the inconsistent prior distributions $p_1(\theta)$ and $p_2(\theta)$.
Using the Product of Experts pooling \eqref{eq:poe} gives the pooled prior to be
\begin{align}
p_\text{pool}(\theta) &\propto \text{Beta}(a, b) \cdot \text{Beta}(c, d) 
                      \propto \theta^{a-1}(1 - \theta)^{b-1} \cdot \theta^{c-1}(1 - \theta)^{d-1} \nonumber \\
                      &= \theta^{a + c - 2}(1 - \theta)^{b + d - 2} \propto \text{Beta}(a + c - 1, b + d - 1). \label{eq:pooledbeta}
\end{align}

By Equation \eqref{eq:meldpost}, the relevant Markov melded posterior is
\begin{align}
p_{\text{meld}}(\theta \, | \, y_1, y_2)
&\propto p_{\text{pool}}(\theta) \prod_{m=1}^{2} \frac{p_m(\theta \, | \, y_m)}{p_m(\theta)}
\propto p_1(\theta)p_2(\theta) \prod_{m=1}^{2} \frac{p_m(\theta \, | \, y_m)}{p_m(\theta)} \nonumber \\
&\propto p_1(\theta \, | \, y_1)p_2(\theta \, | \, y_2) \nonumber \\
&\propto \text{Beta}(\tau_1 + a + n_2 + c - 1, n_1m-\tau_1 + b + \tau_2 + d - 1). \label{eq:meldedbeta}
\end{align}
In fact, in general under Product of Experts pooling, the melded posterior is proportional to the product of the submodel posteriors
\begin{align}
p_{\text{meld}}(\phi, \psi_1, \ldots, \psi_M \, | \, y_1, \cdots, y_M) 
&\propto p_{\text{pool}}(\phi) \prod_{m=1}^{M} \frac{p_m(\phi, \psi_m, y_m)}{p_m(\phi)} \nonumber \\
&= \prod_{m=1}^{M} p_m(\phi, \psi_m, y_m) \nonumber \\
&\propto \prod_{m=1}^{M} p_m(\phi, \psi_m \, | \, y_m). \label{eq:poecancel}
\end{align}

Figure \ref{fig:ex1} shows that both the melded prior \eqref{eq:pooledbeta} and Markov melded posterior \eqref{eq:meldedbeta} occupy a centrist position between the two submodels.

Suppose rather than Product of Experts pooling we use logarithmic pooling with weights $w_m$ for submodel $m$ such that $w_1 + w_2 = 1$.
The pooled prior is then
\begin{align}
p_\text{pool}(\theta) &\propto \text{Beta}(a, b)^{w_1} \cdot \text{Beta}(c, d)^{w_2}
                      \propto \theta^{w_1(a-1)}(1 - \theta)^{w_1(b-1)} \cdot \theta^{w_2(c-1)}(1 - \theta)^{w_2(d-1)} \nonumber \\
                      &\propto \text{Beta}(w_1a + w_2c - w_1 - w_2 + 1, w_1b + w_2d - w_1 - w_2 + 1)
                      \propto \text{Beta}(e, f),
\end{align}
where $e = w_1a + w_2c - w_1 - w_2 + 1$ and $f = w_1b + w_2d - w_1 - w_2 + 1$.
Using this pooled prior, the Markov melded posterior is given by
\begin{align}
p_{\text{meld}}(\theta \, | \, y_1, y_2)
&\propto p_{\text{pool}}(\theta) \prod_{m=1}^{2} \frac{p_m(\theta \, | \, y_m)}{p_m(\theta)} \nonumber \\
&\propto \text{Beta}(e, f) 
\frac{\text{Beta}(\tau_1 + a, n_1m-\tau_1 + b)}{\text{Beta}(a, b)} \frac{\text{Beta}(n_2 + c, \tau_2 + d)}{\text{Beta}(c, d)} \nonumber \\
&\propto \theta^{e + \tau_1 + n_2 - 1}
         (1 - \theta)^{f + n_1m - \tau_1 + \tau_2 - 1} \nonumber \\
&\propto \text{Beta}(e + \tau_1 + n_2, f + n_1m - \tau_1 + \tau_2)
\end{align}
Note that setting $w_1 = w_2 = 1$ gives, as in Equation \eqref{eq:meldedbeta}, a $\text{Beta}(a + c + \tau_1 + n_2 - 1, b + d + n_1m - \tau_1 + \tau_2 - 1)$ distribution.
Figure \ref{fig:ex1weight} shows the results of Markov melding for three weight settings. 
In parts (a) and (c) the weighting amounts to dictatorial pooling to submodels one and two respectively. 
Part (b), in contrast, is a geometric mean between the two submodels.
Under these circumstances of the particular pooling is of little consequence to the Markov melded posterior. \hfill $\square$

```{r dev = "tikz", echo=FALSE, fig.height = 3, fig.cap=paste("Markov melding using logarithmically pooled priors with various weights. The weighting $w_1$, with $w_2 = 1 - w_1$, as well as the posterior mean $\\hat \\theta = \\mathbb{E}[\\theta \\, | \\, y_1, y_2]$ is given above each plot. \\label{fig:ex1weight}")}
weight_plot <- function(w1, w2, sub) {
  s1 <- w1*a + w2*c - w1 - w2 + tau1 + n2 + 1
  s2 <- w1*b + w2*d - w1 - w2 + n1*m - tau1 + tau2 + 1
ggplot(data = data.frame(x = c(0, 1)), aes(x)) +
  stat_function(fun = dbeta, n = 100, 
                args = list(shape1 = s1, 
                            shape2 = s2),
                geom = "area", aes(fill = "1"), alpha = 0.6) +
  stat_function(fun = dbeta, n = 100, 
                args = list(shape1 = w1*a + w2*c - w1 - w2 + 1, 
                            shape2 = w1*b + w2*d - w1 - w2 + 1),
                geom = "area", aes(fill = "2"), alpha = 0.6) +
    geom_vline(xintercept = theta, colour="grey20", linetype = "longdash") +
    labs(x = "$\\theta$", y = "$p(\\theta)$", subtitle = sub) +
    scale_fill_manual(values = c(darkpink, lightpink)) +
    
    theme(panel.background = element_blank(),
          legend.position = "none",
          axis.line = element_line(colour = "grey"))
}

p1 <- weight_plot(1, 0, sub = "(a) $w_1 = 1, \\hat \\theta = 0.444$")
p2 <- weight_plot(0.5, 0.5, sub = "(b) $w_1 = 0.5, \\hat \\theta = 0.457$")
p3 <- weight_plot(0, 1, sub = "$(c) w_1 = 0, \\hat \\theta = 0.469$")

cowplot::plot_grid(p1, p2, p3, ncol = 3)
```

Example 2.2 shows that, for a simple tractable case, Markov melding behaves as one would hope.
